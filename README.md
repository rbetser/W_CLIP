# Whitened CLIP as a Likelihood Surrogate of Images and Captions

[![Paper](https://img.shields.io/badge/arXiv-2505.06934-b31b1b)](https://arxiv.org/abs/2505.06934)
[![License: CC BY 4.0](https://img.shields.io/badge/License-CC%20BY%204.0-lightgrey.svg)](https://creativecommons.org/licenses/by/4.0/)
[![Python](https://img.shields.io/badge/python-3.8%2B-blue)](https://www.python.org/)

> **Official repository** for the paper:  
> ğŸ“„ **W_CLIP: Whitening-Enhanced Likelihood for Image-Text Embedding Alignment**  
> Accepted to the **42nd International Conference on Machine Learning (ICML 2025)**
> [[arXiv:2505.06934](https://arxiv.org/abs/2505.06934)]

---

## ğŸ§  Overview

**W_CLIP** introduces a **training-free** method to estimate the likelihood of images and captions using CLIP embeddings.  
By applying a **whitening transformation** to the CLIP latent space, the method standardizes embeddings to have zero mean, unit variance, and no inter-feature correlation.  
This transformation results in an identity covariance matrix, allowing each embedding to approximate a multivariate standard normal distribution.  
Consequently, the log-likelihood of a sample can be directly estimated from the squared Euclidean norm of its whitened embedding â€” enabling efficient and direct likelihood scoring for both images and texts.

### ğŸ”‘ Key Contributions

- **Whitened CLIP (W_CLIP):**  
  An invertible linear transformation applied to CLIP embeddings, yielding whitened features with zero mean and identity covariance.

- **Statistical Validation:**  
  Empirically validated the Gaussianity of whitened embeddings using statistical tests (Anderson-Darling, Dâ€™Agostino-Pearson), supporting their suitability for likelihood estimation.

- **Direct Likelihood Estimation:**  
  Enables closed-form computation of likelihoods from the norm of whitened features, offering a principled alternative to black-box scoring methods.

- **Experiments:**  
  Demonstrated effectiveness on multiple tasks:
  - Detecting artifacts in generated images.
  - Assessing distribution shifts (e.g., ImageNet-C/R).
  - Enabaling full-circle SLERP for image interpolation.

<p align="center">
  <img src="method_fig.jpg" width="1000" alt="Whitening improves CLIP alignment">
</p>

---
## ğŸ““ Colab Notebooks

You can explore the core functionality of W_CLIP directly in your browser via Google Colab:

| Notebook | Description | Link |
|----------|-------------|------|
| `calc_log_like_image_demo.ipynb` | Computes log-likelihood for given images | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/rbetser/W_CLIP/blob/main/calc_log_like_image_demo.ipynb) |
| `calc_log_like_text_demo.ipynb` | Computes log-likelihood of text prompts | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/rbetser/W_CLIP/blob/main/calc_log_like_text_demo.ipynb) |

---

## ğŸ“¦ Installation

Clone the repo and install dependencies:

```bash
git clone https://github.com/rbetser/W_CLIP.git
cd W_CLIP
```

## ğŸ§© Requirements

This project requires the following packages:

- [PyTorch](https://pytorch.org/) (with GPU support recommended)
- [OpenAI CLIP](https://github.com/openai/CLIP)
- [ğŸ¤— Transformers](https://github.com/huggingface/transformers)
- [ğŸ¤— Diffusers](https://github.com/huggingface/diffusers)

Install with:

```bash
pip install git+https://github.com/openai/CLIP.git
pip install transformers diffusers
```

> ğŸ’¡ **Note:** The `transformers` and `diffusers` libraries are needed only for the full-circle SLERP functionality.

> ğŸ’¡ **Note:** The full-circle SLERP functionality uses the **UnCLIP framework** via `diffusers.pipelines.unclip.UnCLIPImageVariationPipeline` from Hugging Face.
---

## ğŸš€ Usage

This reposirtory provides scripts to compute log-likelihoods for images and texts using CLIP embeddings, compute whitening matrices, and perform full-circle SLERP using the UnCLIP framework.

### 1. Compute Log-Likelihood for Images

```bash
python calc_log_like_image_demo.py
```

### 2. Compute Log-Likelihood for Text

```bash
python calc_log_like_text_demo.py
```

> âš ï¸ **Note:** Image paths, text prompts, and model configurations are currently hardcoded in the demo scripts. Modify the scripts directly to change inputs.  
> âœ… Precomputed whitening matrices and mean vectors are available in the `w_mats/` folder and are required for log-likelihood computations.

### 3. Compute Whitening Matrix using a Set of Images or Text Captions

```bash
python whitening_code.py
```

This script computes a whitening matrix (`w_mat`) and mean vector (`mu`) from a set of CLIP embeddings extracted from either images or text prompts.  
Both outputs are saved to the `w_mats/` directory and are required for likelihood estimation.

> âš ï¸ The input data (images or texts) used for computing whitening is currently defined in the script and should be edited manually.

### 4. Full-Circle SLERP with UnCLIP

```bash
python circle_slerp.py
```

> ğŸ’¡ **Note:** The full-circle SLERP functionality uses the `UnCLIPImageVariationPipeline` from ğŸ¤— Hugging Face's `diffusers` library. Ensure that both `diffusers` and `transformers` are installed.

---

## ğŸ“ Repository Structure

```
W_CLIP/
â”œâ”€â”€ images_dogs/                 # Sample dog images for testing
â”œâ”€â”€ images_hands/                # Sample hand images for testing
â”œâ”€â”€ w_mats/                      # Precomputed whitening matrices
â”œâ”€â”€ calc_log_like_image_demo.py # Script to compute image log-likelihood
â”œâ”€â”€ calc_log_like_text_demo.py  # Script to compute text log-likelihood
â”œâ”€â”€ circle_slerp.py             # Full-circle SLERP using UnCLIP
â”œâ”€â”€ whitening_code.py           # Whitening matrix computation
â””â”€â”€ README.md                   # Project documentation
```

Each script is self-contained and can be run independently.

---

## ğŸ“š Citation

If you use this code or find it helpful, please cite:

```bibtex
@inproceedings{betser2025whitened,
  title={Whitened CLIP as a Likelihood Surrogate of Images and Captions},
  author={Betser, Roy and Levi, Meir Yossef and Gilboa, Guy},
  booktitle={Proceedings of the 42nd International Conference on Machine Learning (ICML)},
  year={2025},
  note={To appear},
  archivePrefix={arXiv},
  eprint={2505.06934}
}
```

---
## ğŸ¥ Presentation Video

ğŸ“½ï¸ [Watch the 5-minute video presentation on YouTube]([https://youtu.be/FIcgHjKhxfI])


---

## ğŸ™‹ Contact

For questions, feedback, or collaborations:

- ğŸ“§ [Roy Betser](mailto:roybe@campus.technion.ac.il)
- ğŸ› Open an issue on [GitHub](https://github.com/rbetser/W_CLIP/issues)
